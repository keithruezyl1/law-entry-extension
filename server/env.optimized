# Optimized Environment Configuration for Villy RAG

# Server
PORT=4000
CORS_ORIGIN=http://localhost:3000

# Postgres
DATABASE_URL=postgres://user:password@localhost:5432/civilify_kb
PGSSL=false

# OpenAI
OPENAI_API_KEY=sk-...
# text-embedding-3-small => 1536 dims (default). If using -large, set vector(3072)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_CHAT_MODEL=gpt-4o

# Authentication
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production

# ===== PERFORMANCE OPTIMIZATIONS =====

# Enhanced Caching (Phase 1)
CHAT_EMBED_TTL_MS=1800000          # 30 minutes (was 5 minutes)
CHAT_EMBED_CACHE_MAX=1000          # 1000 entries (was 200)
CHAT_SQG_TTL_MS=3600000            # 1 hour (was 10 minutes)
CHAT_SQG_CACHE_MAX=2000            # 2000 entries (was 300)
CHAT_CROSS_ENCODER_TTL_MS=1800000  # 30 minutes (was 10 minutes)
CHAT_CROSS_ENCODER_CACHE_MAX=2000  # 2000 entries (was 500)

# Early Termination (Phase 2)
CHAT_EARLY_TERMINATION_THRESHOLD=0.85  # Skip reranking if confidence > 0.85
CHAT_SIMPLE_QUERY_SKIP_SQG=true       # Skip SQG for simple queries

# RAG tuning
CHAT_TOP_K=12
CHAT_SIM_THRESHOLD=0.20
CHAT_CONF_THRESHOLD=0.18

# Structured Query Generation (SQG)
CHAT_USE_SQG=true
CHAT_SQG_MODEL=gpt-4o-mini

# ===== IMPROVEMENTS (Phase 1: Zero-Risk) =====

# 1. METADATA FILTERING
# Filter vector search by entry type, legal topics, jurisdiction (based on SQG)
# - Improves precision by 10-20%
# - Zero cost, zero latency overhead
# - Safe: just adds WHERE clauses to existing queries
# Set to 'false' if metadata filtering is too strict and missing results
CHAT_USE_METADATA_FILTERING=true
CHAT_METADATA_MIN_TOPICS=3

# 2. RERANKING
# Mode: 'none', 'cross-encoder', 'llm', 'hybrid'
# - cross-encoder: Fast (50-200ms), local, accurate (recommended)
# - llm: Slower (2-5s), uses OpenAI API, more flexible
# - hybrid: Try cross-encoder first, fallback to LLM
# - none: No reranking (default for backward compatibility)
CHAT_RERANK_MODE=cross-encoder

# Cross-Encoder Reranking (fast, local, accurate)
CHAT_CROSS_ENCODER_HIGH_CONF=0.85    # Skip reranking if confidence > this
CHAT_CROSS_ENCODER_LOW_CONF=0.22     # Skip reranking if confidence < this
CHAT_CROSS_ENCODER_MAX_CANDIDATES=24 # Max candidates to rerank
CHAT_CROSS_ENCODER_TOP_N=8           # Return top N after reranking
CHAT_CROSS_ENCODER_MIN_SIM=0.15      # Min similarity to consider
CHAT_CROSS_ENCODER_BLEND=0.7         # Blend weight (0.7 = 70% cross-encoder, 30% original)

# LLM-based Reranking (slower, but more flexible - legacy)
CHAT_USE_RERANKER=false
CHAT_RERANK_MODEL=gpt-4o-mini
CHAT_RERANK_MODEL_STRONG=gpt-4o
CHAT_RERANK_HIGH_CONF=0.85
CHAT_RERANK_LOW_CONF=0.22
CHAT_RERANK_MAX_CANDIDATES=24
CHAT_RERANK_TOP_N=8
CHAT_RERANK_MIN_VEC=0.0
CHAT_RERANK_MIN_LEX=0.0
CHAT_RERANK_TTL_MS=3600000
CHAT_RERANK_CACHE_MAX=2000

# 3. HNSW INDEX (Hierarchical Navigable Small World)
# HNSW index is created by migration 017_hnsw_index.sql
# Provides 95%+ recall vs. 80-85% for IVFFlat
# PostgreSQL automatically chooses best index per query (dual-index mode)
# 
# HNSW query parameter (requires pgvector 0.5.0+)
CHAT_HNSW_EF_SEARCH=40  # Higher = better recall, slower query (default: 40)
# 
# IVFFlat query parameter (for comparison)
CHAT_IVF_PROBES=8       # Number of lists to search (default: 8-10)
#
# To verify HNSW is active, check server logs for:
# "[vector] HNSW index active, ef_search = 40"
# or run: SELECT indexname FROM pg_indexes WHERE tablename = 'kb_entries' AND indexname LIKE '%hnsw%';

# ===== PERFORMANCE MONITORING =====

# Enable performance logging
CHAT_PERFORMANCE_LOGGING=true

# Batch processing (for future implementation)
CHAT_BATCH_PROCESSING=false
CHAT_BATCH_SIZE=10

# ===== EXPERIMENTAL FEATURES =====

# Context memoization (Phase 3)
CHAT_CONTEXT_MEMOIZATION=true
CHAT_CONTEXT_CACHE_TTL_MS=1800000  # 30 minutes
CHAT_CONTEXT_CACHE_MAX=5000        # 5000 entries

# Smart query routing (Phase 3)
CHAT_SMART_ROUTING=true
CHAT_ROUTING_THRESHOLD=0.8

