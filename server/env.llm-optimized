# Optimized Environment Configuration for Villy RAG - LLM Optimizations

# Server
PORT=4000
CORS_ORIGIN=http://localhost:3000

# Postgres
DATABASE_URL=postgres://user:password@localhost:5432/civilify_kb
PGSSL=false

# OpenAI - LLM Optimizations
OPENAI_API_KEY=sk-...
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_CHAT_MODEL=gpt-4o-mini  # Faster model for generation

# Authentication
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production

# ===== LLM PERFORMANCE OPTIMIZATIONS =====

# 1. FASTER MODEL
# Use gpt-4o-mini instead of gpt-4o for 3-5x faster generation
OPENAI_CHAT_MODEL=gpt-4o-mini

# 2. RESPONSE CACHING
# Cache LLM responses for identical queries
CHAT_RESPONSE_TTL_MS=3600000          # 1 hour cache
CHAT_RESPONSE_CACHE_MAX=500           # 500 cached responses

# 3. TOKEN LIMITS
# Limit response length to reduce generation time
CHAT_MAX_TOKENS=2000                  # Limit response to 2000 tokens
CHAT_MAX_SOURCES=6                    # Limit context to 6 sources

# 4. STREAMING (Optional)
# Enable streaming for better perceived performance
CHAT_USE_STREAMING=false              # Set to true for streaming responses

# 5. PERFORMANCE MONITORING
# Track LLM generation time separately
CHAT_PERFORMANCE_LOGGING=true

# ===== EXISTING PERFORMANCE OPTIMIZATIONS =====

# Enhanced Caching (Phase 1)
CHAT_EMBED_TTL_MS=1800000          # 30 minutes (was 5 minutes)
CHAT_EMBED_CACHE_MAX=1000          # 1000 entries (was 200)
CHAT_SQG_TTL_MS=3600000            # 1 hour (was 10 minutes)
CHAT_SQG_CACHE_MAX=2000            # 2000 entries (was 300)
CHAT_CROSS_ENCODER_TTL_MS=1800000  # 30 minutes (was 10 minutes)
CHAT_CROSS_ENCODER_CACHE_MAX=2000  # 2000 entries (was 500)

# Early Termination (Phase 2)
CHAT_EARLY_TERMINATION_THRESHOLD=0.85  # Skip reranking if confidence > 0.85
CHAT_SIMPLE_QUERY_SKIP_SQG=true       # Skip SQG for simple queries

# RAG tuning
CHAT_TOP_K=12
CHAT_SIM_THRESHOLD=0.20
CHAT_CONF_THRESHOLD=0.18

# Structured Query Generation (SQG)
CHAT_USE_SQG=true
CHAT_SQG_MODEL=gpt-4o-mini

# ===== IMPROVEMENTS (Phase 1: Zero-Risk) =====

# 1. METADATA FILTERING
CHAT_USE_METADATA_FILTERING=true
CHAT_METADATA_MIN_TOPICS=3

# 2. RERANKING
CHAT_RERANK_MODE=cross-encoder

# Cross-Encoder Reranking (fast, local, accurate)
CHAT_CROSS_ENCODER_HIGH_CONF=0.85    # Skip reranking if confidence > this
CHAT_CROSS_ENCODER_LOW_CONF=0.22     # Skip reranking if confidence < this
CHAT_CROSS_ENCODER_MAX_CANDIDATES=24 # Max candidates to rerank
CHAT_CROSS_ENCODER_TOP_N=8           # Return top N after reranking
CHAT_CROSS_ENCODER_MIN_SIM=0.15      # Min similarity to consider
CHAT_CROSS_ENCODER_BLEND=0.7         # Blend weight (0.7 = 70% cross-encoder, 30% original)

# LLM-based Reranking (slower, but more flexible - legacy)
CHAT_USE_RERANKER=false
CHAT_RERANK_MODEL=gpt-4o-mini
CHAT_RERANK_MODEL_STRONG=gpt-4o
CHAT_RERANK_HIGH_CONF=0.85
CHAT_RERANK_LOW_CONF=0.22
CHAT_RERANK_MAX_CANDIDATES=24
CHAT_RERANK_TOP_N=8
CHAT_RERANK_MIN_VEC=0.0
CHAT_RERANK_MIN_LEX=0.0
CHAT_RERANK_TTL_MS=3600000
CHAT_RERANK_CACHE_MAX=2000

# 3. HNSW INDEX (Hierarchical Navigable Small World)
CHAT_HNSW_EF_SEARCH=40  # Higher = better recall, slower query (default: 40)
CHAT_IVF_PROBES=8       # Number of lists to search (default: 8-10)

# ===== PERFORMANCE MONITORING =====

# Enable performance logging
CHAT_PERFORMANCE_LOGGING=true

# Batch processing (for future implementation)
CHAT_BATCH_PROCESSING=false
CHAT_BATCH_SIZE=10

# ===== EXPERIMENTAL FEATURES =====

# Context memoization (Phase 3)
CHAT_CONTEXT_MEMOIZATION=true
CHAT_CONTEXT_CACHE_TTL_MS=1800000  # 30 minutes
CHAT_CONTEXT_CACHE_MAX=5000        # 5000 entries

# Smart query routing (Phase 3)
CHAT_SMART_ROUTING=true
CHAT_ROUTING_THRESHOLD=0.8

